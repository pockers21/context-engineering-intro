# llama.cpp å¼€å‘æ¨¡å¼æŒ‡å—

åŸºäºçœŸå®llama.cppä»£ç åˆ†æçš„å¼€å‘æ¨¡å¼æ€»ç»“ã€‚

## ğŸ—ï¸ é¡¹ç›®æ¶æ„æ¨¡å¼

### ç›®å½•ç»“æ„çº¦å®š
```
llama.cpp/
â”œâ”€â”€ src/                    # æ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ llama.cpp          # ä¸»APIå®ç°
â”‚   â”œâ”€â”€ llama-*.cpp        # åŠŸèƒ½æ¨¡å—
â”‚   â””â”€â”€ llama-*.h          # æ¨¡å—å¤´æ–‡ä»¶
â”œâ”€â”€ ggml/                  # å¼ é‡æ“ä½œåº“
â”‚   â”œâ”€â”€ include/           # å…¬å…±å¤´æ–‡ä»¶
â”‚   â””â”€â”€ src/              # å®ç°æ–‡ä»¶
â”œâ”€â”€ examples/              # ç¤ºä¾‹ç¨‹åº
â”œâ”€â”€ tests/                 # å•å…ƒæµ‹è¯•
â””â”€â”€ tools/                # å·¥å…·ç¨‹åº
```

### æ¨¡å—åˆ†ç¦»åŸåˆ™
- **åŠŸèƒ½æ¨¡å—åŒ–**: æ¯ä¸ªæ¨¡å—è´Ÿè´£ç‰¹å®šåŠŸèƒ½ï¼ˆå¦‚quantã€vocabã€modelç­‰ï¼‰
- **æ¥å£æ¸…æ™°**: å…¬å…±APIåœ¨include/ç›®å½•ï¼Œå†…éƒ¨å®ç°åœ¨src/
- **ä¾èµ–æœ€å°åŒ–**: æ¨¡å—é—´ä¾èµ–å…³ç³»æ¸…æ™°ï¼Œé¿å…å¾ªç¯ä¾èµ–

## ğŸ”§ ç¼–ç æ¨¡å¼

### 1. é”™è¯¯å¤„ç†æ¨¡å¼

```cpp
// ä½¿ç”¨LLAMA_LOG_*å®è®°å½•æ—¥å¿—
LLAMA_LOG_ERROR("%s: failed to load model\n", __func__);
LLAMA_LOG_WARN("%s: unsupported feature\n", __func__);
LLAMA_LOG_INFO("%s: model loaded successfully\n", __func__);

// å‡½æ•°è¿”å›å€¼çº¦å®š
bool load_model(const std::string& path) {
    if (path.empty()) {
        LLAMA_LOG_ERROR("%s: empty path\n", __func__);
        return false;
    }
    // ... å®ç°
    return true;
}

// å¼‚å¸¸å¤„ç†ï¼ˆä»…åœ¨æ„é€ å‡½æ•°ä¸­ä½¿ç”¨ï¼‰
class model_loader {
public:
    explicit model_loader(const std::string& path) {
        if (!load(path)) {
            throw std::runtime_error("failed to load model");
        }
    }
};
```

### 2. å†…å­˜ç®¡ç†æ¨¡å¼

```cpp
// RAIIèµ„æºç®¡ç†
class cuda_context {
public:
    cuda_context() {
        CHECK_CUDA_ERROR(cudaStreamCreate(&stream_));
    }
    
    ~cuda_context() {
        if (stream_) {
            cudaStreamDestroy(stream_);
        }
    }
    
    // ç¦ç”¨æ‹·è´ï¼Œå…è®¸ç§»åŠ¨
    cuda_context(const cuda_context&) = delete;
    cuda_context& operator=(const cuda_context&) = delete;
    cuda_context(cuda_context&&) = default;
    cuda_context& operator=(cuda_context&&) = default;
    
private:
    cudaStream_t stream_ = nullptr;
};

// æ™ºèƒ½æŒ‡é’ˆä½¿ç”¨
std::unique_ptr<ggml_context, decltype(&ggml_free)> 
    ctx(ggml_init(params), &ggml_free);
```

### 3. å‘½åçº¦å®š

```cpp
// å‡½æ•°å’Œå˜é‡: snake_case
int n_vocab = 0;
float compute_score(const float* data, int size);

// ç±»å‹: snake_case with suffix
struct llama_model {};
enum ggml_type {};

// å¸¸é‡: UPPER_CASE
#define LLAMA_MAX_DEVICES 16
const int QK_K = 256;

// ç§æœ‰æˆå‘˜: trailing underscore
class example {
private:
    int value_;
    std::string name_;
};
```

### 4. å‚æ•°éªŒè¯æ¨¡å¼

```cpp
static size_t quantize_row(const float * src, void * dst, int n) {
    // è¾“å…¥éªŒè¯
    if (!src || !dst) {
        LLAMA_LOG_ERROR("%s: null pointer\n", __func__);
        return 0;
    }
    
    if (n <= 0 || n % QK_K != 0) {
        LLAMA_LOG_ERROR("%s: invalid size %d\n", __func__, n);
        return 0;
    }
    
    // å®ç°...
    return processed_bytes;
}
```

## âš¡ æ€§èƒ½ä¼˜åŒ–æ¨¡å¼

### 1. SIMDä½¿ç”¨æ¨¡å¼

```cpp
// ç¼–è¯‘æ—¶ç‰¹æ€§æ£€æµ‹
#if defined(__AVX2__)
static void process_avx2(const float* src, float* dst, int n) {
    const int n_vec = n & ~7; // 8ä¸ªfloatä¸€ç»„
    
    for (int i = 0; i < n_vec; i += 8) {
        __m256 v = _mm256_loadu_ps(src + i);
        // å¤„ç†...
        _mm256_storeu_ps(dst + i, v);
    }
    
    // å¤„ç†å‰©ä½™å…ƒç´ 
    for (int i = n_vec; i < n; ++i) {
        dst[i] = process_scalar(src[i]);
    }
}
#endif

// è¿è¡Œæ—¶åˆ†å‘
void process_data(const float* src, float* dst, int n) {
#if defined(__AVX2__)
    if (ggml_cpu_has_avx2()) {
        process_avx2(src, dst, n);
        return;
    }
#endif
    process_fallback(src, dst, n);
}
```

### 2. å†…å­˜è®¿é—®æ¨¡å¼

```cpp
// ç¼“å­˜å‹å¥½çš„å¾ªç¯é¡ºåº
void matrix_multiply(const float* A, const float* B, float* C, 
                    int M, int N, int K) {
    // åˆ†å—å¤„ç†ï¼Œä¼˜åŒ–ç¼“å­˜å‘½ä¸­ç‡
    const int BLOCK_SIZE = 64;
    
    for (int ii = 0; ii < M; ii += BLOCK_SIZE) {
        for (int jj = 0; jj < N; jj += BLOCK_SIZE) {
            for (int kk = 0; kk < K; kk += BLOCK_SIZE) {
                // å†…æ ¸è®¡ç®—
                int i_max = std::min(ii + BLOCK_SIZE, M);
                int j_max = std::min(jj + BLOCK_SIZE, N);
                int k_max = std::min(kk + BLOCK_SIZE, K);
                
                for (int i = ii; i < i_max; ++i) {
                    for (int j = jj; j < j_max; ++j) {
                        float sum = 0.0f;
                        for (int k = kk; k < k_max; ++k) {
                            sum += A[i*K + k] * B[k*N + j];
                        }
                        C[i*N + j] += sum;
                    }
                }
            }
        }
    }
}
```

### 3. å¹¶è¡ŒåŒ–æ¨¡å¼

```cpp
// OpenMPå¹¶è¡ŒåŒ–
static void compute_parallel(const float* input, float* output, 
                            int n_elements, int n_threads) {
    #pragma omp parallel for num_threads(n_threads)
    for (int i = 0; i < n_elements; ++i) {
        output[i] = expensive_computation(input[i]);
    }
}

// æ‰‹åŠ¨çº¿ç¨‹æ± 
class thread_pool {
    std::vector<std::thread> workers;
    std::queue<std::function<void()>> tasks;
    std::mutex queue_mutex;
    std::condition_variable condition;
    bool stop;

public:
    explicit thread_pool(size_t threads) : stop(false) {
        for (size_t i = 0; i < threads; ++i) {
            workers.emplace_back([this] {
                // å·¥ä½œçº¿ç¨‹å®ç°...
            });
        }
    }
};
```

## ğŸ§ª æµ‹è¯•æ¨¡å¼

### 1. å•å…ƒæµ‹è¯•ç»“æ„

```cpp
// tests/test-quantize-fns.cpp æ¨¡å¼
#include "ggml.h"
#undef NDEBUG
#include <assert.h>

static const char* RESULT_STR[] = {"ok", "FAILED"};

// æµ‹è¯•æ•°æ®ç”Ÿæˆ
static void generate_data(float offset, size_t n, float * dst) {
    for (size_t i = 0; i < n; i++) {
        dst[i] = 0.1f + 2.0f * cosf(i + offset);
    }
}

// ç²¾åº¦éªŒè¯
static float calculate_rmse(const float* a, const float* b, size_t n) {
    double sum = 0.0;
    for (size_t i = 0; i < n; i++) {
        double diff = a[i] - b[i];
        sum += diff * diff;
    }
    return sqrtf(sum / n);
}

// æµ‹è¯•å‡½æ•°
static bool test_quantization_accuracy() {
    const size_t test_size = 1024;
    std::vector<float> src(test_size);
    std::vector<uint8_t> quantized(test_size);
    std::vector<float> dst(test_size);
    
    generate_data(0.0f, test_size, src.data());
    
    // é‡åŒ–å’Œåé‡åŒ–
    quantize_row_q4_0(src.data(), quantized.data(), test_size);
    dequantize_row_q4_0(quantized.data(), dst.data(), test_size);
    
    // éªŒè¯ç²¾åº¦
    float rmse = calculate_rmse(src.data(), dst.data(), test_size);
    return rmse < MAX_QUANTIZATION_ERROR;
}

int main() {
    bool success = true;
    
    success &= test_quantization_accuracy();
    success &= test_edge_cases();
    success &= test_performance();
    
    printf("Overall result: %s\n", RESULT_STR[!success]);
    return success ? 0 : 1;
}
```

### 2. æ€§èƒ½æµ‹è¯•æ¨¡å¼

```cpp
#include <chrono>

static void benchmark_operation() {
    const int iterations = 1000;
    const int data_size = 4096;
    
    std::vector<float> input(data_size);
    std::vector<float> output(data_size);
    
    // é¢„çƒ­
    for (int i = 0; i < 10; ++i) {
        test_operation(input.data(), output.data(), data_size);
    }
    
    // åŸºå‡†æµ‹è¯•
    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < iterations; ++i) {
        test_operation(input.data(), output.data(), data_size);
    }
    auto end = std::chrono::high_resolution_clock::now();
    
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    double throughput = (double)(iterations * data_size) / duration.count(); // MB/s
    
    printf("Throughput: %.2f MB/s\n", throughput);
}
```

## ğŸ”Œ æ‰©å±•æ¨¡å¼

### 1. æ–°æ“ä½œæ·»åŠ æ¨¡å¼

```cpp
// 1. åœ¨ggml.hä¸­å£°æ˜
GGML_API struct ggml_tensor * ggml_new_operation(
    struct ggml_context * ctx,
    struct ggml_tensor  * a);

// 2. åœ¨ggml.cä¸­å®ç°
struct ggml_tensor * ggml_new_operation_impl(
    struct ggml_context * ctx,
    struct ggml_tensor  * a) {
    
    // éªŒè¯è¾“å…¥
    GGML_ASSERT(a);
    
    // åˆ›å»ºç»“æœå¼ é‡
    struct ggml_tensor * result = ggml_new_tensor(ctx, a->type, a->n_dims, a->ne);
    
    // è®¾ç½®æ“ä½œç±»å‹
    result->op = GGML_OP_NEW_OPERATION;
    result->src[0] = a;
    
    return result;
}

// 3. æ·»åŠ åˆ°è®¡ç®—å›¾æ‰§è¡Œ
static void ggml_compute_forward_new_operation_f32(
    const struct ggml_compute_params * params,
    const struct ggml_tensor * src0,
    struct ggml_tensor * dst) {
    
    // å®ç°å…·ä½“è®¡ç®—é€»è¾‘
}
```

### 2. åç«¯é›†æˆæ¨¡å¼

```cpp
// åç«¯æ³¨å†Œ
static struct ggml_backend_i new_backend_i = {
    /* .get_name                = */ ggml_backend_new_name,
    /* .free                    = */ ggml_backend_new_free,
    /* .get_default_buffer_type = */ ggml_backend_new_get_default_buffer_type,
    /* .set_tensor              = */ ggml_backend_new_set_tensor,
    /* .get_tensor              = */ ggml_backend_new_get_tensor,
    /* .cpy_tensor_from         = */ ggml_backend_new_cpy_tensor_from,
    /* .cpy_tensor_to           = */ ggml_backend_new_cpy_tensor_to,
    /* .graph_plan_create       = */ ggml_backend_new_graph_plan_create,
    /* .graph_plan_free         = */ ggml_backend_new_graph_plan_free,
    /* .graph_plan_compute      = */ ggml_backend_new_graph_plan_compute,
    /* .supports_op             = */ ggml_backend_new_supports_op,
};
```

## ğŸ“¦ æ„å»ºç³»ç»Ÿæ¨¡å¼

### CMakeæ¨¡å¼

```cmake
# æ¡ä»¶ç¼–è¯‘
option(LLAMA_NEW_FEATURE "Enable new feature" OFF)

if (LLAMA_NEW_FEATURE)
    add_compile_definitions(LLAMA_NEW_FEATURE)
    list(APPEND LLAMA_SOURCES src/llama-new-feature.cpp)
endif()

# ä¾èµ–æŸ¥æ‰¾
find_package(NewLibrary QUIET)
if (NewLibrary_FOUND)
    target_link_libraries(llama PRIVATE NewLibrary::NewLibrary)
    add_compile_definitions(LLAMA_USE_NEW_LIBRARY)
endif()

# æµ‹è¯•æ·»åŠ 
if (LLAMA_BUILD_TESTS)
    add_executable(test-new-feature tests/test-new-feature.cpp)
    target_link_libraries(test-new-feature PRIVATE llama)
    add_test(NAME test-new-feature COMMAND test-new-feature)
endif()
```

è¿™äº›æ¨¡å¼åŸºäºçœŸå®çš„llama.cppä»£ç åˆ†æå¾—å‡ºï¼Œéµå¾ªè¿™äº›æ¨¡å¼å¯ä»¥ç¡®ä¿ä½ çš„ä»£ç ä¸é¡¹ç›®é£æ ¼ä¿æŒä¸€è‡´ï¼Œå¹¶è·å¾—æœ€ä½³çš„æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚