#!/usr/bin/env python3
"""
å¿«é€Ÿè®ºæ–‡åˆ†æå·¥å…· - é‡ç‚¹åˆ†ææœ€å…³é”®çš„è®ºæ–‡
"""

import subprocess
import re
import json
from datetime import datetime

def get_paper_info(url):
    """å¿«é€Ÿè·å–è®ºæ–‡åŸºç¡€ä¿¡æ¯"""
    try:
        cmd = f'curl -s -m 10 "{url}"'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        content = result.stdout
        
        # æå–æ ‡é¢˜
        title_match = re.search(r'<title>(.*?)</title>', content)
        title = title_match.group(1).strip() if title_match else "æœªè·å–åˆ°æ ‡é¢˜"
        title = re.sub(r'\[.*?\]\s*', '', title)  # ç§»é™¤ [2408.11743] è¿™æ ·çš„å‰ç¼€
        
        # æå–æ‘˜è¦
        abstract_match = re.search(r'<div class="ltx_abstract">.*?<p[^>]*>(.*?)</p>', content, re.DOTALL)
        abstract = ""
        if abstract_match:
            abstract = re.sub(r'<[^>]+>', '', abstract_match.group(1))
            abstract = re.sub(r'\s+', ' ', abstract).strip()
        
        # æŸ¥æ‰¾æ€§èƒ½æ•°æ®
        performance = []
        text_for_perf = abstract.lower()
        
        # æŸ¥æ‰¾åŠ é€Ÿæ¯”
        speedup_matches = re.findall(r'(\d+\.?\d*)\s*[Ã—x]\s*(?:speedup|faster|improvement)', text_for_perf)
        performance.extend([f"{m}xåŠ é€Ÿ" for m in speedup_matches])
        
        # æŸ¥æ‰¾ç™¾åˆ†æ¯”
        percent_matches = re.findall(r'(\d+)%\s*(?:improvement|better|reduction|faster)', text_for_perf)
        performance.extend([f"{m}%æ”¹è¿›" for m in percent_matches])
        
        return {
            "title": title,
            "abstract": abstract,
            "performance": performance,
            "url": url
        }
    except Exception as e:
        return {"title": f"è·å–å¤±è´¥: {e}", "abstract": "", "performance": [], "url": url}

def analyze_key_papers():
    """åˆ†æå…³é”®è®ºæ–‡"""
    
    # é‡ç‚¹è®ºæ–‡åˆ—è¡¨ (ä»paper.mdä¸­æå–çš„å…³é”®é“¾æ¥)
    key_papers = [
        "https://arxiv.org/html/2409.16997v1",  # INT-FlashAttention
        "https://arxiv.org/html/2410.02367v1",  # SageAttention  
        "https://arxiv.org/html/2410.00161v2",  # KV-Compress
        "https://arxiv.org/abs/2408.11743",     # MARLIN
        "https://arxiv.org/html/2408.14690v1",  # Training-Free Activation Sparsity
        "https://arxiv.org/html/2410.09426v1",  # FlatQuant
        "https://arxiv.org/abs/2405.03917",     # KV Cache 1-bit
        "https://arxiv.org/html/2410.11305v1",  # QSpec
        "https://arxiv.org/html/2503.01840",    # EAGLE-3 (ä½ æåˆ°çš„)
        "https://flashinfer.ai/2025/03/10/sampling.html"  # Sorting-Free
    ]
    
    print("ğŸš€ å¿«é€Ÿåˆ†æå…³é”®è®ºæ–‡")
    print("="*50)
    
    results = []
    
    for i, url in enumerate(key_papers, 1):
        print(f"[{i}/{len(key_papers)}] åˆ†æ: {url}")
        
        info = get_paper_info(url)
        
        # åŸºäºçœŸå®å†…å®¹è¯„ä¼°
        title = info['title']
        abstract = info['abstract']
        
        # æŠ€æœ¯åˆ†ç±»
        tech_category = "å…¶ä»–"
        if any(word in title.lower() + abstract.lower() for word in ['flash', 'attention']):
            tech_category = "æ³¨æ„åŠ›ä¼˜åŒ–"
        elif any(word in title.lower() + abstract.lower() for word in ['quantiz', 'int8', 'int4']):
            tech_category = "é‡åŒ–æŠ€æœ¯"  
        elif any(word in title.lower() + abstract.lower() for word in ['kv', 'cache']):
            tech_category = "KVç¼“å­˜ä¼˜åŒ–"
        elif any(word in title.lower() + abstract.lower() for word in ['speculative', 'eagle']):
            tech_category = "æ¨æµ‹é‡‡æ ·"
        elif any(word in title.lower() + abstract.lower() for word in ['sparse', 'sparsity']):
            tech_category = "ç¨€ç–åŒ–"
        
        # llama.cppç›¸å…³æ€§è¯„åˆ†
        relevance = 0.3  # åŸºç¡€åˆ†
        if any(word in title.lower() + abstract.lower() for word in 
               ['inference', 'generation', 'llm', 'transformer']):
            relevance += 0.3
        if any(word in title.lower() + abstract.lower() for word in
               ['gpu', 'cuda', 'memory', 'efficient']):
            relevance += 0.2
        if any(word in title.lower() + abstract.lower() for word in
               ['quantization', 'attention', 'cache']):
            relevance += 0.2
        relevance = min(relevance, 1.0)
        
        # å®æ–½å¤æ‚åº¦ä¼°ç®—
        complexity = "ä¸­ç­‰"
        if any(word in title.lower() + abstract.lower() for word in
               ['novel', 'new', 'training', 'architecture']):
            complexity = "é«˜"
        elif any(word in title.lower() + abstract.lower() for word in
                 ['plug-and-play', 'compatible', 'efficient']):
            complexity = "ä½"
        
        result = {
            "title": title,
            "url": url,
            "category": tech_category,
            "abstract_snippet": abstract[:200] + "..." if len(abstract) > 200 else abstract,
            "performance_claims": info['performance'],
            "llama_cpp_relevance": relevance,
            "estimated_complexity": complexity,
            "analysis_status": "æˆåŠŸ" if abstract else "éƒ¨åˆ†æˆåŠŸ"
        }
        
        results.append(result)
        
        print(f"âœ… {title[:50]}...")
        print(f"   åˆ†ç±»: {tech_category} | ç›¸å…³æ€§: {relevance:.2f} | å¤æ‚åº¦: {complexity}")
        print(f"   æ€§èƒ½: {', '.join(info['performance'][:2]) if info['performance'] else 'å¾…ç¡®è®¤'}")
        print()
    
    return results

def generate_priority_ranking(results):
    """ç”Ÿæˆä¼˜å…ˆçº§æ’åº"""
    def calculate_score(paper):
        score = paper['llama_cpp_relevance'] * 5  # ç›¸å…³æ€§æƒé‡
        
        # æ€§èƒ½æ”¶ç›ŠåŠ åˆ†
        if paper['performance_claims']:
            score += len(paper['performance_claims']) * 1.5
        
        # å¤æ‚åº¦è°ƒæ•´
        if paper['estimated_complexity'] == 'ä½':
            score += 1
        elif paper['estimated_complexity'] == 'é«˜':
            score -= 1
        
        # æŠ€æœ¯é‡è¦æ€§åŠ åˆ†
        important_categories = ['æ³¨æ„åŠ›ä¼˜åŒ–', 'KVç¼“å­˜ä¼˜åŒ–', 'é‡åŒ–æŠ€æœ¯']
        if paper['category'] in important_categories:
            score += 1
        
        return score
    
    # è®¡ç®—å¾—åˆ†å¹¶æ’åº
    for paper in results:
        paper['priority_score'] = calculate_score(paper)
    
    results.sort(key=lambda x: x['priority_score'], reverse=True)
    return results

def main():
    # åˆ†æå…³é”®è®ºæ–‡
    results = analyze_key_papers()
    
    # ç”Ÿæˆä¼˜å…ˆçº§æ’åº
    ranked_results = generate_priority_ranking(results)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        "analysis_timestamp": datetime.now().isoformat(),
        "method": "åŸºäºçœŸå®è®ºæ–‡å†…å®¹çš„å¿«é€Ÿåˆ†æ",
        "papers_analyzed": len(results),
        "priority_ranking": ranked_results,
        "summary": {
            "total_papers": len(results),
            "successful_analysis": len([r for r in results if r['analysis_status'] == 'æˆåŠŸ']),
            "top_categories": {}
        }
    }
    
    # ç»Ÿè®¡æŠ€æœ¯åˆ†ç±»
    categories = {}
    for paper in results:
        cat = paper['category']
        categories[cat] = categories.get(cat, 0) + 1
    report['summary']['top_categories'] = categories
    
    # ä¿å­˜ç»“æœ
    with open('/root/llama.cpp-clip/PRPs/paper_analysis/results/quick_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("="*50)
    print("ğŸ¯ å¿«é€Ÿåˆ†æå®Œæˆ!")
    print(f"ğŸ“Š æˆåŠŸåˆ†æ: {report['summary']['successful_analysis']}/{report['summary']['total_papers']} ç¯‡è®ºæ–‡")
    print("\nğŸ† TOP 5 ä¼˜å…ˆçº§æ’åº:")
    
    for i, paper in enumerate(ranked_results[:5], 1):
        print(f"{i}. {paper['title'][:55]}...")
        print(f"   å¾—åˆ†: {paper['priority_score']:.1f} | {paper['category']} | å¤æ‚åº¦: {paper['estimated_complexity']}")
        if paper['performance_claims']:
            print(f"   æ€§èƒ½: {', '.join(paper['performance_claims'][:2])}")
        print()
    
    print("ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: quick_analysis.json")

if __name__ == "__main__":
    main()