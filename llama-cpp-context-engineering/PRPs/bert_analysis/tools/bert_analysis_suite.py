#!/usr/bin/env python3
"""
BERTæŠ€æœ¯æœºåˆ¶æ·±åº¦åˆ†æå¥—ä»¶
åŸºäºContext Engineeringæ–¹æ³•è®ºæ„å»ºçš„ç»“æ„åŒ–åˆ†æå·¥å…·

åŒ…å«æ¨¡å—:
1. AttentionMaskAnalyzer - attention maskæœºåˆ¶åˆ†æ
2. MLMFlowTracer - MLM forward passè¿½è¸ª  
3. LossCalculationVerifier - æŸå¤±è®¡ç®—éªŒè¯
4. ShapeTransformationTracker - å½¢çŠ¶å˜åŒ–è¿½è¸ª
5. ComprehensiveReporter - ç»¼åˆæŠ¥å‘Šç”Ÿæˆ
"""

import torch
import torch.nn.functional as F
from torch.nn import CrossEntropyLoss
import sys
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# ç¡®ä¿ä½¿ç”¨æœ¬åœ°transformersæºç 
sys.path.insert(0, '/root/transformers/src')
from transformers import BertTokenizer, BertForMaskedLM

class BERTAnalysisSuite:
    """BERTæŠ€æœ¯æœºåˆ¶ç»¼åˆåˆ†æå¥—ä»¶"""
    
    def __init__(self, model_path='/root/autodl-fs/google-bert/bert-base-chinese'):
        self.model_path = model_path
        self.tokenizer = BertTokenizer.from_pretrained(model_path)
        self.model = BertForMaskedLM.from_pretrained(model_path)
        self.analysis_results = {}
        self.log_entries = []
        
    def log(self, message: str, level: str = "INFO"):
        """ç»Ÿä¸€æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {level}: {message}"
        self.log_entries.append(log_entry)
        print(log_entry)
    
    def verify_environment(self):
        """éªŒè¯åˆ†æç¯å¢ƒ"""
        self.log("=== ç¯å¢ƒéªŒè¯ ===")
        self.log(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        self.log(f"è¯æ±‡è¡¨å¤§å°: {self.model.config.vocab_size}")
        self.log(f"éšè—å±‚ç»´åº¦: {self.model.config.hidden_size}")
        self.log(f"æ³¨æ„åŠ›å¤´æ•°: {self.model.config.num_attention_heads}")
        
        self.analysis_results['environment'] = {
            'model_path': self.model_path,
            'vocab_size': self.model.config.vocab_size,
            'hidden_size': self.model.config.hidden_size,
            'num_attention_heads': self.model.config.num_attention_heads
        }
    
    def analyze_attention_mask(self, test_text: str):
        """Attention Maskå®Œæ•´åˆ†æ"""
        self.log("=== Phase 2: Attention Maskæœºåˆ¶åˆ†æ ===")
        
        inputs = self.tokenizer(test_text, return_tensors="pt")
        input_ids = inputs.input_ids
        attention_mask = inputs.attention_mask
        
        # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
        analyzer = AttentionMaskAnalyzer(self)
        
        mask_results = analyzer.analyze_mask_transformation(input_ids, attention_mask)
        attention_results = analyzer.trace_attention_computation(input_ids, attention_mask)
        
        self.analysis_results['attention_mask'] = {
            'mask_transformation': mask_results,
            'attention_computation': attention_results
        }
    
    def trace_mlm_forward_pass(self, test_text: str):
        """MLM Forward Passè¿½è¸ª"""
        self.log("=== Phase 3: MLM Forward Passè¿½è¸ª ===")
        
        tracer = MLMFlowTracer(self)
        flow_results = tracer.trace_complete_forward(test_text)
        
        self.analysis_results['mlm_forward_pass'] = flow_results
    
    def verify_loss_calculation(self, test_text: str):
        """æŸå¤±è®¡ç®—éªŒè¯"""
        self.log("=== Phase 4: æŸå¤±è®¡ç®—éªŒè¯ ===")
        
        verifier = LossCalculationVerifier(self)
        loss_results = verifier.verify_ignore_index_mechanism()
        
        self.analysis_results['loss_calculation'] = loss_results
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        self.log("=== Phase 5: ç”Ÿæˆç»¼åˆæŠ¥å‘Š ===")
        
        report = {
            'analysis_timestamp': datetime.now().isoformat(),
            'summary': {
                'attention_mask_understood': True,
                'mlm_flow_traced': True,
                'loss_mechanism_verified': self.analysis_results.get('loss_calculation', {}).get('verification_passed', False)
            },
            'key_findings': [
                "Attention maskçš„'å…¨0'ç°è±¡æ˜¯æ­£ç¡®çš„ï¼Œå› ä¸ºç»è¿‡(1.0-mask)*(-inf)è½¬æ¢",
                "MLMçš„[MASK]tokenåœ¨attentionä¸­æ˜¯å¯è§çš„ï¼Œåªåœ¨æŸå¤±è®¡ç®—æ—¶è¢«é€‰æ‹©æ€§å¤„ç†",
                "CrossEntropyLossçš„ignore_index=-100æœºåˆ¶ç¡®ä¿åªæœ‰çœŸå®æ ‡ç­¾ä½ç½®å‚ä¸è®­ç»ƒ"
            ],
            'log_entries': self.log_entries
        }
        
        self.analysis_results['comprehensive_report'] = report
        self.log("âœ… ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    def run_comprehensive_analysis(self, test_text: str = "æˆ‘å–œæ¬¢å­¦ä¹ äººå·¥æ™ºèƒ½"):
        """è¿è¡Œå®Œæ•´çš„BERTæŠ€æœ¯åˆ†æ"""
        self.log("å¼€å§‹BERTæŠ€æœ¯æœºåˆ¶æ·±åº¦åˆ†æ", "INFO")
        self.log(f"åˆ†ææ–‡æœ¬: {test_text}")
        
        # Phase 1: ç¯å¢ƒéªŒè¯
        self.verify_environment()
        
        # Phase 2: Attention Maskåˆ†æ
        self.analyze_attention_mask(test_text)
        
        # Phase 3: MLM Forward Passè¿½è¸ª
        self.trace_mlm_forward_pass(test_text)
        
        # Phase 4: æŸå¤±è®¡ç®—éªŒè¯
        self.verify_loss_calculation(test_text)
        
        # Phase 5: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report()
        
        return self.analysis_results

class AttentionMaskAnalyzer:
    """Attention Maskæœºåˆ¶æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self, suite: BERTAnalysisSuite):
        self.suite = suite
        
    def analyze_mask_transformation(self, input_ids, attention_mask):
        """åˆ†æattention maskçš„æ•°å€¼è½¬æ¢è¿‡ç¨‹"""
        self.suite.log("=== Attention Maskæ•°å€¼è½¬æ¢åˆ†æ ===")
        
        results = {
            'original_mask': attention_mask.clone(),
            'mask_values': {},
            'transformation_steps': []
        }
        
        # 1. åŸå§‹mask
        self.suite.log(f"1. åŸå§‹attention_mask: {attention_mask[0].tolist()}")
        self.suite.log(f"   å«ä¹‰: 1=æœ‰æ•ˆtoken, 0=padding")
        
        # 2. æ¨¡æ‹Ÿextend_attention_maskè¿‡ç¨‹
        batch_size, seq_length = attention_mask.shape
        extended_mask = attention_mask[:, None, None, :]  # [batch, 1, 1, seq_len]
        self.suite.log(f"2. æ‰©å±•ä¸º4D: {extended_mask.shape}")
        
        # 3. å…³é”®è½¬æ¢: (1.0 - mask) * -inf
        dtype = torch.float32
        extended_mask = (1.0 - extended_mask) * torch.finfo(dtype).min
        self.suite.log(f"3. è½¬æ¢å…¬å¼: (1.0 - mask) * {torch.finfo(dtype).min}")
        self.suite.log(f"   è½¬æ¢åå€¼: {extended_mask[0, 0, 0, :5].tolist()}")
        self.suite.log(f"   å«ä¹‰: 0=å¯å…³æ³¨, -inf=å¿½ç•¥")
        
        results['extended_mask'] = extended_mask
        results['transformation_formula'] = "(1.0 - attention_mask) * torch.finfo(dtype).min"
        
        return results
        
    def trace_attention_computation(self, input_ids, attention_mask):
        """è¿½è¸ªattentionè®¡ç®—ä¸­maskçš„ä½œç”¨"""
        self.suite.log("=== Attentionè®¡ç®—ä¸­Maskä½œç”¨è¿½è¸ª ===")
        
        # è·å–ç¬¬ä¸€å±‚attentionçš„ä¸­é—´å€¼
        with torch.no_grad():
            # æ‰‹åŠ¨è°ƒç”¨embeddingå’Œç¬¬ä¸€å±‚
            embeddings = self.suite.model.bert.embeddings(input_ids)
            
            # è·å–æ‰©å±•åçš„attention mask
            extended_mask = self.suite.model.bert.get_extended_attention_mask(
                attention_mask, input_ids.shape
            )
            
            # ç¬¬ä¸€å±‚attention
            attention_layer = self.suite.model.bert.encoder.layer[0].attention.self
            
            # è®¡ç®—Q, K, V
            mixed_query = attention_layer.query(embeddings)
            mixed_key = attention_layer.key(embeddings)
            mixed_value = attention_layer.value(embeddings)
            
            # è½¬æ¢ä¸ºå¤šå¤´æ ¼å¼
            query = attention_layer.transpose_for_scores(mixed_query)
            key = attention_layer.transpose_for_scores(mixed_key)
            
            # è®¡ç®—attention scores
            attention_scores = torch.matmul(query, key.transpose(-1, -2))
            attention_scores = attention_scores / (attention_layer.attention_head_size ** 0.5)
            
            self.suite.log(f"attention_scoresè®¡ç®—å: {attention_scores[0, 0, 0, :5]}")
            
            # åº”ç”¨mask
            attention_scores_masked = attention_scores + extended_mask
            self.suite.log(f"åŠ ä¸Šmaskå: {attention_scores_masked[0, 0, 0, :5]}")
            
            # softmax
            attention_probs = F.softmax(attention_scores_masked, dim=-1)
            self.suite.log(f"softmaxå: {attention_probs[0, 0, 0, :5]}")
            
        return {
            'raw_scores': attention_scores[0, 0, 0, :].detach(),
            'masked_scores': attention_scores_masked[0, 0, 0, :].detach(), 
            'attention_probs': attention_probs[0, 0, 0, :].detach()
        }

class MLMFlowTracer:
    """MLM Forward Passå®Œæ•´è¿½è¸ªå™¨"""
    
    def __init__(self, suite: BERTAnalysisSuite):
        self.suite = suite
        
    def trace_complete_forward(self, text: str):
        """è¿½è¸ªå®Œæ•´çš„MLM forward pass"""
        self.suite.log("=== MLM Forward Passå®Œæ•´è¿½è¸ª ===")
        
        # å‡†å¤‡è¾“å…¥
        inputs = self.suite.tokenizer(text, return_tensors="pt")
        input_ids = inputs.input_ids
        attention_mask = inputs.attention_mask
        
        shapes_log = []
        
        def log_shape(name: str, tensor: torch.Tensor):
            shape_info = f"{name}: {list(tensor.shape)}"
            shapes_log.append(shape_info)
            self.suite.log(f"  {shape_info}")
        
        self.suite.model.eval()
        with torch.no_grad():
            # 1. Embeddings
            self.suite.log("1. BertEmbeddings")
            embeddings_out = self.suite.model.bert.embeddings(input_ids)
            log_shape("embeddings_output", embeddings_out)
            
            # 2. Encoder layers
            self.suite.log("2. BertEncoderå¤„ç†")
            encoder_out = self.suite.model.bert.encoder(
                embeddings_out, 
                attention_mask=self.suite.model.bert.get_extended_attention_mask(
                    attention_mask, input_ids.shape
                )
            )
            sequence_output = encoder_out.last_hidden_state
            log_shape("sequence_output", sequence_output)
            
            # 3. Prediction Head Transform
            self.suite.log("3. BertLMPredictionHead")
            self.suite.log("  3.1 Transformå±‚")
            transformed = self.suite.model.cls.predictions.transform(sequence_output)
            log_shape("transformed_output", transformed)
            
            # 3.2 Decoder
            self.suite.log("  3.2 Decoderçº¿æ€§å±‚")
            prediction_scores = self.suite.model.cls.predictions.decoder(transformed)
            log_shape("prediction_scores", prediction_scores)
            
        return {
            'shapes_log': shapes_log,
            'sequence_output_shape': list(sequence_output.shape),
            'prediction_scores_shape': list(prediction_scores.shape),
            'vocab_size': self.suite.model.config.vocab_size
        }

class LossCalculationVerifier:
    """æŸå¤±è®¡ç®—æœºåˆ¶éªŒè¯å™¨"""
    
    def __init__(self, suite: BERTAnalysisSuite):
        self.suite = suite
        
    def verify_ignore_index_mechanism(self):
        """éªŒè¯CrossEntropyLossçš„ignore_index=-100æœºåˆ¶"""
        self.suite.log("=== CrossEntropyLoss ignore_indexæœºåˆ¶éªŒè¯ ===")
        
        # æ¨¡æ‹Ÿæ•°æ®
        batch_size, seq_length, vocab_size = 1, 8, 21128
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        prediction_scores = torch.randn(batch_size, seq_length, vocab_size)
        labels = torch.full((batch_size, seq_length), -100, dtype=torch.long)
        
        # åªåœ¨ä½ç½®5è®¾ç½®çœŸå®æ ‡ç­¾
        mask_pos = 5
        true_token_id = 3721  # éšæœºé€‰æ‹©ä¸€ä¸ªtoken id
        labels[0, mask_pos] = true_token_id
        
        self.suite.log(f"labels: {labels[0].tolist()}")
        self.suite.log(f"åªæœ‰ä½ç½®{mask_pos}æœ‰çœŸå®æ ‡ç­¾: {true_token_id}")
        
        # 1. ä½¿ç”¨PyTorchçš„CrossEntropyLoss
        loss_fct = CrossEntropyLoss()  # ignore_index=-100 æ˜¯é»˜è®¤å€¼
        pytorch_loss = loss_fct(
            prediction_scores.view(-1, vocab_size), 
            labels.view(-1)
        )
        
        # 2. æ‰‹åŠ¨è®¡ç®—åªæœ‰maskä½ç½®çš„æŸå¤±
        mask_logits = prediction_scores[0, mask_pos]  # [vocab_size]
        mask_probs = F.softmax(mask_logits, dim=-1)
        manual_loss = -torch.log(mask_probs[true_token_id])
        
        self.suite.log(f"PyTorch CrossEntropyLoss: {pytorch_loss.item():.6f}")
        self.suite.log(f"æ‰‹åŠ¨è®¡ç®—æŸå¤±: {manual_loss.item():.6f}")
        self.suite.log(f"å·®å¼‚: {abs(pytorch_loss.item() - manual_loss.item()):.8f}")
        
        # 3. éªŒè¯å…¶ä»–ä½ç½®è¢«å¿½ç•¥
        self.suite.log("\néªŒè¯-100ä½ç½®è¢«å¿½ç•¥:")
        for pos in [0, 1, 2, 3, 4, 6, 7]:
            pos_labels = torch.full((batch_size, seq_length), -100, dtype=torch.long)
            pos_labels[0, pos] = true_token_id
            pos_loss = loss_fct(prediction_scores.view(-1, vocab_size), pos_labels.view(-1))
            self.suite.log(f"  ä½ç½®{pos}å•ç‹¬æŸå¤±: {pos_loss.item():.6f}")
            
        return {
            'pytorch_loss': pytorch_loss.item(),
            'manual_loss': manual_loss.item(),
            'difference': abs(pytorch_loss.item() - manual_loss.item()),
            'verification_passed': abs(pytorch_loss.item() - manual_loss.item()) < 1e-6
        }

def main():
    """ä¸»åˆ†ææµç¨‹"""
    print("ğŸš€ å¯åŠ¨BERTæŠ€æœ¯æœºåˆ¶æ·±åº¦åˆ†æ")
    print("åŸºäºContext Engineeringæ–¹æ³•è®ºçš„ç»“æ„åŒ–åˆ†æ")
    print("="*60)
    
    # åˆ›å»ºåˆ†æå¥—ä»¶
    suite = BERTAnalysisSuite()
    
    # è¿è¡Œç»¼åˆåˆ†æ
    results = suite.run_comprehensive_analysis("æˆ‘å–œæ¬¢å­¦ä¹ äººå·¥[MASK]")
    
    print("\n" + "="*60)
    print("ğŸ¯ åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° bert_analysis_results.json")
    
    # ä¿å­˜ç»“æœ
    with open('/root/bert_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)

if __name__ == "__main__":
    main()