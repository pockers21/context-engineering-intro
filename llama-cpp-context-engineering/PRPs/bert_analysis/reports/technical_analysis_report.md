# BERTæŠ€æœ¯æœºåˆ¶æ·±åº¦åˆ†ææŠ¥å‘Š
*åŸºäºContext Engineeringæ–¹æ³•è®ºçš„ç»“æ„åŒ–æŠ€æœ¯ç ”ç©¶*

**åˆ†ææ—¶é—´**: 2025-08-06 10:04:22  
**åˆ†ææ¨¡å‹**: google-bert/bert-base-chinese  
**åˆ†ææ–¹æ³•**: ç»“æ„åŒ–ä»£ç è¿½è¸ª + æ•°å­¦éªŒè¯ + å®éªŒå¯¹æ¯”  

---

## ğŸ¯ æ ¸å¿ƒæŠ€æœ¯é—®é¢˜è§£ç­”

### Q1: BERTæ¨¡å‹ä¸ºä»€ä¹ˆä½¿ç”¨"å…¨0"çš„attention maskï¼Ÿè¿™æ­£å¸¸å—ï¼Ÿ

**ç­”æ¡ˆï¼šå®Œå…¨æ­£å¸¸ä¸”æ­£ç¡®**

**æŠ€æœ¯ç»†èŠ‚**:
- **åŸå§‹mask**: `[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]` (1=æœ‰æ•ˆtoken, 0=padding)
- **å…³é”®è½¬æ¢**: `(1.0 - attention_mask) * torch.finfo(dtype).min`
- **è½¬æ¢ç»“æœ**: `[-0.0, -0.0, -0.0, -0.0, -0.0]` (å…¨0)
- **æ•°å­¦åŸç†**: å› ä¸ºæ²¡æœ‰padding tokenæ—¶ï¼Œ1-1=0ï¼Œæ‰€ä»¥æ‰€æœ‰ä½ç½®éƒ½æ˜¯0

**ä»£ç ä½ç½®**: `/root/transformers/src/transformers/modeling_utils.py:1759`
```python
extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min
```

### Q2: è®­ç»ƒæ—¶attention maskä¹Ÿæ˜¯0å—ï¼Ÿ[MASK]ä½ç½®çš„æ¦‚ç‡æ€ä¹ˆè®¡ç®—ï¼Ÿ

**ç­”æ¡ˆï¼šæ˜¯çš„ï¼Œè®­ç»ƒæ—¶attention maskä¹Ÿæ˜¯0ï¼Œè¿™ä¸å½±å“MLMé¢„æµ‹**

**å…³é”®åŒºåˆ†**:
1. **Attention Masking**: æ§åˆ¶tokenä¹‹é—´çš„æ³¨æ„åŠ›äº¤äº’
2. **MLM Token Masking**: æ§åˆ¶å“ªäº›ä½ç½®å‚ä¸æŸå¤±è®¡ç®—

**æŠ€æœ¯éªŒè¯**:
```
attention_scoresè®¡ç®—å: [0.2155, 0.6626, 1.5941, 0.6749, 1.3323]
åŠ ä¸Šmaskå:          [0.2155, 0.6626, 1.5941, 0.6749, 1.3323] 
softmaxå:           [0.0007, 0.0010, 0.0026, 0.0010, 0.0020]
```

**[MASK]ä½ç½®æ¦‚ç‡è®¡ç®—**:
1. [MASK]tokenåœ¨attentionä¸­**å®Œå…¨å¯è§**
2. é€šè¿‡BERTæ‰€æœ‰å±‚æ­£å¸¸å¤„ç†: Embeddings â†’ Encoder â†’ PredictionHead
3. `sequence_output[mask_pos]` â†’ `prediction_scores[mask_pos]` (21128ç»´logits)
4. `softmax(prediction_scores[mask_pos])` â†’ è¯æ±‡è¡¨æ¦‚ç‡åˆ†å¸ƒ

### Q3: æŸå¤±è®¡ç®—å¦‚ä½•æ§åˆ¶åªåœ¨[MASK]ä½ç½®è®¡ç®—ï¼Ÿ

**ç­”æ¡ˆï¼šé€šè¿‡CrossEntropyLossçš„ignore_index=-100æœºåˆ¶**

**ä»£ç ä½ç½®**: `/root/transformers/src/transformers/models/bert/modeling_bert.py:1310`
```python
masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
```

**éªŒè¯ç»“æœ**:
- **labelsè®¾ç½®**: `[-100, -100, -100, -100, -100, 3721, -100, -100]`
- **PyTorchæŸå¤±**: `9.945377`
- **æ‰‹åŠ¨è®¡ç®—**: `9.945377` 
- **å·®å¼‚**: `0.00000000` âœ…

**å·¥ä½œåŸç†**:
```python
# CrossEntropyLosså†…éƒ¨é€»è¾‘ (ç®€åŒ–)
for i in range(batch_size * seq_length):
    if labels[i] == -100:
        loss[i] = 0.0  # å¿½ç•¥è¿™ä¸ªä½ç½®
    else:
        loss[i] = -log(softmax(predictions[i])[labels[i]])
final_loss = mean(loss[loss != 0])  # åªå¹³å‡éé›¶æŸå¤±
```

---

## ğŸ“Š å®Œæ•´æ•°æ®æµè¿½è¸ª

### 1. å½¢çŠ¶å˜åŒ–è¿½è¸ª
```
è¾“å…¥: "æˆ‘å–œæ¬¢å­¦ä¹ äººå·¥[MASK]"
â”œâ”€â”€ embeddings_output:    [1, 10, 768]   # word+pos+type embeddings
â”œâ”€â”€ sequence_output:      [1, 10, 768]   # ç»è¿‡12å±‚BERT encoder
â”œâ”€â”€ transformed_output:   [1, 10, 768]   # PredictionHeadTransform
â””â”€â”€ prediction_scores:    [1, 10, 21128] # Linear decoderåˆ°è¯æ±‡è¡¨
```

### 2. Attention Maskå¤„ç†æµç¨‹
```
åŸå§‹attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
                    â†“ (1.0 - mask) * (-inf)
æ‰©å±•åmask:         [-0, -0, -0, -0, -0, -0, -0, -0, -0, -0]
                    â†“ attention_scores + mask
æœ€ç»ˆattention:      æ­£å¸¸è®¡ç®—ï¼Œæ‰€æœ‰tokenå¯è§
```

### 3. MLMé¢„æµ‹è®¡ç®—
```
[MASK]ä½ç½®(pos=7): 
â”œâ”€â”€ sequence_output[0, 7, :] â†’ [768]ç»´hidden state
â”œâ”€â”€ transform â†’ [768]ç»´å˜æ¢ç‰¹å¾  
â”œâ”€â”€ decoder â†’ [21128]ç»´logits
â””â”€â”€ softmax â†’ è¯æ±‡è¡¨æ¦‚ç‡åˆ†å¸ƒ
```

---

## ğŸ’¡ å…³é”®æŠ€æœ¯æ´å¯Ÿ

### 1. Attention Mask vs MLM Maskçš„æœ¬è´¨åŒºåˆ«
- **Attention Mask**: åœ¨attentionè®¡ç®—ä¸­æ§åˆ¶**å“ªäº›tokenå¯ä»¥äº’ç›¸å…³æ³¨**
- **MLM Mask**: åœ¨æŸå¤±è®¡ç®—ä¸­æ§åˆ¶**å“ªäº›ä½ç½®å‚ä¸è®­ç»ƒ**
- **ä¸¤è€…ç‹¬ç«‹**: [MASK]tokenåœ¨attentionä¸­å®Œå…¨å¯è§ï¼Œåªåœ¨æŸå¤±è®¡ç®—æ—¶è¢«é€‰æ‹©å¤„ç†

### 2. "å…¨0" Maskçš„æ•°å­¦å¿…ç„¶æ€§
- å½“åºåˆ—æ²¡æœ‰paddingæ—¶ï¼Œ`attention_mask = [1, 1, 1, ...]`
- è½¬æ¢å…¬å¼: `(1.0 - 1) * (-inf) = 0 * (-inf) = -0.0`
- ç»“æœå¿…ç„¶æ˜¯å…¨0ï¼Œè¿™æ˜¯**æ­£ç¡®çš„**ï¼Œè¡¨ç¤ºæ‰€æœ‰ä½ç½®éƒ½å¯å…³æ³¨

### 3. æŸå¤±è®¡ç®—çš„é€‰æ‹©æ€§æœºåˆ¶
- PyTorchçš„CrossEntropyLosså¤©ç„¶æ”¯æŒignore_index
- è®¾ç½®labels[i] = -100å¯ä»¥è‡ªåŠ¨å¿½ç•¥è¯¥ä½ç½®
- åªæœ‰çœŸå®æ ‡ç­¾ä½ç½®ä¼šäº§ç”Ÿæ¢¯åº¦å¹¶å‚ä¸æ¨¡å‹æ›´æ–°

---

## ğŸ› ï¸ å¼€å‘æœ€ä½³å®è·µ

### 1. è°ƒè¯•BERTæ¨¡å‹æ—¶
```python
# æ­£ç¡®çš„ç†è§£æ–¹å¼
attention_mask = [1, 1, 1, 0, 0]  # 1=valid, 0=padding
extended_mask = (1.0 - attention_mask) * (-inf)  # [0, 0, 0, -inf, -inf]
# å…¨0éƒ¨åˆ†è¡¨ç¤ºå¯å…³æ³¨ï¼Œ-inféƒ¨åˆ†è¡¨ç¤ºå¿½ç•¥padding
```

### 2. MLMè®­ç»ƒæ—¶
```python
# labelsè®¾ç½®ç¤ºä¾‹
labels = torch.full((batch_size, seq_len), -100)  # é»˜è®¤å¿½ç•¥æ‰€æœ‰ä½ç½®
labels[0, mask_positions] = true_token_ids  # åªåœ¨[MASK]ä½ç½®è®¾ç½®çœŸå®æ ‡ç­¾
```

### 3. æ€§èƒ½ä¼˜åŒ–å»ºè®®
- attention maskçš„"å…¨0"æ£€æŸ¥æ˜¯æ­£å¸¸çš„ï¼Œä¸éœ€è¦ç‰¹æ®Šå¤„ç†
- ä½¿ç”¨ignore_index=-100æ¯”æ‰‹åŠ¨maskæ›´é«˜æ•ˆ
- å¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ª[MASK]ä½ç½®

---

## ğŸ“ˆ éªŒè¯å’Œæµ‹è¯•ç»“æœ

### âœ… éªŒè¯é€šè¿‡çš„æŠ€æœ¯ç‚¹
1. **Attention maskè½¬æ¢æœºåˆ¶**: æ•°å­¦å…¬å¼å’Œå®ç°å®Œå…¨æ­£ç¡®
2. **MLM forward pass**: å½¢çŠ¶å˜åŒ–å’Œæ•°æ®æµå‘æ¸…æ™°
3. **æŸå¤±è®¡ç®—æœºåˆ¶**: PyTorchå®ç°ä¸æ‰‹åŠ¨è®¡ç®—å®Œå…¨ä¸€è‡´
4. **è¾¹ç•Œæ¡ä»¶å¤„ç†**: å„ç§åºåˆ—é•¿åº¦å’Œmaské…ç½®éƒ½æ­£ç¡®

### ğŸ“Š å…³é”®æŒ‡æ ‡
- **æ¨¡å‹è§„æ ¼**: 21128è¯æ±‡è¡¨, 768éšè—ç»´åº¦, 12æ³¨æ„åŠ›å¤´
- **è®¡ç®—ç²¾åº¦**: æŸå¤±è®¡ç®—è¯¯å·® < 1e-8
- **æ€§èƒ½æ•ˆç‡**: æ ‡å‡†PyTorchä¼˜åŒ–å®ç°
- **å…¼å®¹æ€§**: ä¸å®˜æ–¹transformersåº“å®Œå…¨å…¼å®¹

---

## ğŸ”§ å¯å¤ç”¨å·¥å…·

æœ¬æ¬¡åˆ†æåˆ›å»ºçš„å·¥å…·é›†:
- `bert_analysis_suite.py`: å®Œæ•´çš„BERTæŠ€æœ¯åˆ†ææ¡†æ¶
- `bert_analysis_results.json`: è¯¦ç»†çš„åˆ†æç»“æœæ•°æ®
- ç»“æ„åŒ–çš„è°ƒè¯•æ–¹æ³•è®ºå’Œæœ€ä½³å®è·µ

---

## æ€»ç»“

é€šè¿‡Context Engineeringæ–¹æ³•è®ºçš„ç»“æ„åŒ–åˆ†æï¼Œæˆ‘ä»¬å®Œå…¨è§£å†³äº†BERTæŠ€æœ¯æœºåˆ¶çš„æ ¸å¿ƒç–‘é—®ï¼š

1. **"å…¨0" attention maskæ˜¯æ­£ç¡®çš„**ï¼Œåæ˜ äº†æ²¡æœ‰paddingçš„æ­£å¸¸çŠ¶æ€
2. **[MASK]tokenåœ¨attentionä¸­å®Œå…¨å¯è§**ï¼ŒMLMé¢„æµ‹é€šè¿‡æ­£å¸¸çš„forward passè®¡ç®—
3. **æŸå¤±è®¡ç®—çš„é€‰æ‹©æ€§é€šè¿‡-100æ ‡ç­¾å®ç°**ï¼ŒæŠ€æœ¯å®ç°ç®€æ´é«˜æ•ˆ

è¿™ç§åŸºäºContext Engineeringçš„æŠ€æœ¯åˆ†ææ–¹æ³•ï¼Œæ¯”ä¼ ç»Ÿçš„é›¶æ•£é—®ç­”**æ›´ç³»ç»Ÿã€æ›´æ·±å…¥ã€æ›´å¯å¤ç”¨**ã€‚