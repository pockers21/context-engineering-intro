#!/usr/bin/env python3
"""
å…¨é¢çš„è®ºæ–‡åˆ†æå·¥å…· - åŸºäºçœŸå®å†…å®¹
ç³»ç»Ÿæ€§åˆ†ææ‰€æœ‰æ¨ç†ä¼˜åŒ–è®ºæ–‡ï¼Œæä¾›å‡†ç¡®çš„æŠ€æœ¯è¯„ä¼°å’Œå®æ–½å»ºè®®
"""

import subprocess
import re
import json
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
import time

@dataclass
class PaperAnalysis:
    title: str
    url: str
    abstract: str
    key_techniques: List[str]
    performance_improvements: List[str]
    implementation_requirements: List[str]
    llama_cpp_compatibility: float
    estimated_complexity: str
    potential_benefits: List[str]
    technical_challenges: List[str]
    priority_score: float

class ComprehensivePaperAnalyzer:
    def __init__(self):
        self.analyses = []
        self.failed_papers = []
    
    def fetch_paper_content(self, url: str) -> Dict[str, str]:
        """è·å–è®ºæ–‡çš„è¯¦ç»†å†…å®¹"""
        try:
            # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            time.sleep(1)
            
            cmd = f'curl -s "{url}"'
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            content = result.stdout
            
            # æå–æ ‡é¢˜
            title = self.extract_title(content)
            
            # æå–æ‘˜è¦
            abstract = self.extract_abstract(content)
            
            # æå–æ­£æ–‡å…³é”®éƒ¨åˆ†
            key_sections = self.extract_key_sections(content)
            
            return {
                "title": title,
                "abstract": abstract,
                "content": content,
                "key_sections": key_sections
            }
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥ {url}: {e}")
            return {}
    
    def extract_title(self, content: str) -> str:
        """æå–è®ºæ–‡æ ‡é¢˜"""
        patterns = [
            r'<title>(.*?)</title>',
            r'<h1[^>]*>(.*?)</h1>',
            r'class="ltx_title"[^>]*>(.*?)</span>'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if match:
                title = re.sub(r'<[^>]+>', '', match.group(1))
                title = re.sub(r'\s+', ' ', title).strip()
                if len(title) > 10:  # ç¡®ä¿ä¸æ˜¯ç©ºæ ‡é¢˜
                    return title
        return "æœªæ‰¾åˆ°æ ‡é¢˜"
    
    def extract_abstract(self, content: str) -> str:
        """æå–è®ºæ–‡æ‘˜è¦"""
        patterns = [
            r'<div class="ltx_abstract">.*?<p[^>]*>(.*?)</p>',
            r'<section[^>]*abstract[^>]*>.*?<p[^>]*>(.*?)</p>',
            r'Abstract</h[0-9]>.*?<p[^>]*>(.*?)</p>'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if match:
                abstract = re.sub(r'<[^>]+>', '', match.group(1))
                abstract = re.sub(r'\s+', ' ', abstract).strip()
                if len(abstract) > 50:  # ç¡®ä¿æ˜¯æœ‰æ„ä¹‰çš„æ‘˜è¦
                    return abstract
        return "æœªæ‰¾åˆ°æ‘˜è¦"
    
    def extract_key_sections(self, content: str) -> Dict[str, str]:
        """æå–è®ºæ–‡å…³é”®ç« èŠ‚"""
        sections = {}
        
        # æŸ¥æ‰¾æ–¹æ³•éƒ¨åˆ†
        method_patterns = [
            r'(?i)<h[0-9][^>]*>\s*method[s]?\s*</h[0-9]>(.*?)(?=<h[0-9]|$)',
            r'(?i)<h[0-9][^>]*>\s*approach\s*</h[0-9]>(.*?)(?=<h[0-9]|$)',
        ]
        
        for pattern in method_patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                sections['method'] = re.sub(r'<[^>]+>', '', match.group(1))[:500]
                break
        
        # æŸ¥æ‰¾å®éªŒç»“æœ
        result_patterns = [
            r'(?i)<h[0-9][^>]*>\s*experiment[s]?\s*</h[0-9]>(.*?)(?=<h[0-9]|$)',
            r'(?i)<h[0-9][^>]*>\s*result[s]?\s*</h[0-9]>(.*?)(?=<h[0-9]|$)',
            r'(?i)<h[0-9][^>]*>\s*evaluation\s*</h[0-9]>(.*?)(?=<h[0-9]|$)'
        ]
        
        for pattern in result_patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                sections['results'] = re.sub(r'<[^>]+>', '', match.group(1))[:500]
                break
        
        return sections
    
    def analyze_techniques(self, title: str, abstract: str, content: str) -> List[str]:
        """åˆ†æè®ºæ–‡çš„å…³é”®æŠ€æœ¯"""
        techniques = []
        text = (title + " " + abstract).lower()
        
        # é‡åŒ–ç›¸å…³æŠ€æœ¯
        if any(word in text for word in ['quantization', 'quantize', 'int8', 'int4', 'bit']):
            if 'int8' in text: techniques.append('INT8é‡åŒ–')
            if 'int4' in text: techniques.append('INT4é‡åŒ–')
            if 'mixed' in text: techniques.append('æ··åˆç²¾åº¦')
            if not any('INT' in t for t in techniques):
                techniques.append('é€šç”¨é‡åŒ–')
        
        # æ³¨æ„åŠ›ä¼˜åŒ–
        if any(word in text for word in ['attention', 'flash', 'sage']):
            if 'flash' in text: techniques.append('FlashAttentionä¼˜åŒ–')
            elif 'sage' in text: techniques.append('SageAttentionæŠ€æœ¯')
            else: techniques.append('æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–')
        
        # ç¼“å­˜ä¼˜åŒ–
        if any(word in text for word in ['cache', 'kv', 'memory']):
            if 'kv' in text: techniques.append('KVç¼“å­˜ä¼˜åŒ–')
            if 'compress' in text: techniques.append('ç¼“å­˜å‹ç¼©')
            if 'paged' in text: techniques.append('åˆ†é¡µç¼“å­˜')
        
        # æ¨æµ‹é‡‡æ ·
        if any(word in text for word in ['speculative', 'eagle', 'draft']):
            techniques.append('æ¨æµ‹é‡‡æ ·')
        
        # ç¨€ç–åŒ–
        if any(word in text for word in ['sparse', 'sparsity', 'pruning']):
            techniques.append('æ¿€æ´»ç¨€ç–åŒ–')
        
        # ç®—å­èåˆ
        if any(word in text for word in ['fusion', 'fuse', 'kernel']):
            techniques.append('ç®—å­èåˆ')
        
        # å¹¶è¡Œæ¨ç†
        if any(word in text for word in ['parallel', 'batch', 'throughput']):
            techniques.append('å¹¶è¡Œæ¨ç†ä¼˜åŒ–')
        
        return techniques if techniques else ['å…¶ä»–ä¼˜åŒ–æŠ€æœ¯']
    
    def extract_performance_data(self, abstract: str, content: str) -> List[str]:
        """æå–æ€§èƒ½æ”¹è¿›æ•°æ®"""
        improvements = []
        text = abstract + " " + content
        
        # æŸ¥æ‰¾åŠ é€Ÿæ¯” (å¦‚ 2.5x, 3x speedup)
        speedup_patterns = [
            r'(\d+\.?\d*)\s*[Ã—x]\s*(?:speedup|faster|improvement)',
            r'(?:speedup|faster|improvement).*?(\d+\.?\d*)\s*[Ã—x]',
            r'(\d+\.?\d*)\s*times\s*faster'
        ]
        
        for pattern in speedup_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                improvements.append(f"{match}x åŠ é€Ÿ")
        
        # æŸ¥æ‰¾ç™¾åˆ†æ¯”æ”¹è¿› (å¦‚ 30% improvement)
        percent_patterns = [
            r'(\d+)%\s*(?:improvement|better|reduction|faster)',
            r'(?:improve|reduce|increase).*?(\d+)%'
        ]
        
        for pattern in percent_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                improvements.append(f"{match}% æ”¹è¿›")
        
        # æŸ¥æ‰¾å†…å­˜ä¼˜åŒ– (å¦‚ 50% memory reduction)
        memory_patterns = [
            r'(\d+)%\s*(?:memory|mem)\s*(?:reduction|saving)',
            r'(?:memory|mem).*?(?:reduce|save).*?(\d+)%'
        ]
        
        for pattern in memory_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                improvements.append(f"{match}% å†…å­˜èŠ‚çœ")
        
        return improvements if improvements else ['æ€§èƒ½æå‡æ•°æ®å¾…ç¡®è®¤']
    
    def assess_implementation_requirements(self, techniques: List[str], abstract: str) -> List[str]:
        """è¯„ä¼°å®æ–½è¦æ±‚"""
        requirements = []
        text = abstract.lower()
        
        # CUDAå¼€å‘éœ€æ±‚
        if any(word in text for word in ['gpu', 'cuda', 'kernel']):
            requirements.append('CUDAå†…æ ¸å¼€å‘')
        
        # ç®—æ³•é‡æ–°å®ç°
        if any(word in text for word in ['novel', 'new', 'propose']):
            requirements.append('æ–°ç®—æ³•å®ç°')
        
        # æ¶æ„ä¿®æ”¹
        if any(word in text for word in ['architecture', 'framework', 'system']):
            requirements.append('æ¶æ„çº§åˆ«ä¿®æ”¹')
        
        # æ¨¡å‹è®­ç»ƒ/å¾®è°ƒ
        if any(word in text for word in ['training', 'fine-tuning', 'calibration']):
            requirements.append('æ¨¡å‹æ ¡å‡†/è®­ç»ƒ')
        
        # ç‰¹å®šç¡¬ä»¶æ”¯æŒ
        if any(word in text for word in ['ampere', 'tensor', 'specific']):
            requirements.append('ç‰¹å®šç¡¬ä»¶æ”¯æŒ')
        
        return requirements if requirements else ['åŸºç¡€å®ç°']
    
    def calculate_llama_cpp_compatibility(self, techniques: List[str], requirements: List[str]) -> float:
        """è®¡ç®—ä¸llama.cppçš„å…¼å®¹æ€§è¯„åˆ† (0-1)"""
        compatibility = 0.5  # åŸºç¡€åˆ†æ•°
        
        # æå‡å…¼å®¹æ€§çš„å› ç´ 
        positive_factors = [
            ('FlashAttentionä¼˜åŒ–', 0.2),
            ('é‡åŒ–', 0.15),
            ('KVç¼“å­˜ä¼˜åŒ–', 0.2),
            ('ç®—å­èåˆ', 0.15),
            ('å¹¶è¡Œæ¨ç†ä¼˜åŒ–', 0.1)
        ]
        
        for technique in techniques:
            for factor, score in positive_factors:
                if factor in technique:
                    compatibility += score
        
        # é™ä½å…¼å®¹æ€§çš„å› ç´ 
        negative_factors = [
            ('æ¶æ„çº§åˆ«ä¿®æ”¹', -0.3),
            ('æ¨¡å‹æ ¡å‡†/è®­ç»ƒ', -0.2),
            ('ç‰¹å®šç¡¬ä»¶æ”¯æŒ', -0.1)
        ]
        
        for requirement in requirements:
            for factor, penalty in negative_factors:
                if factor in requirement:
                    compatibility += penalty
        
        return max(0.0, min(1.0, compatibility))
    
    def estimate_complexity(self, techniques: List[str], requirements: List[str]) -> str:
        """ä¼°ç®—å®æ–½å¤æ‚åº¦"""
        complexity_score = 0
        
        # æŠ€æœ¯å¤æ‚åº¦
        high_complexity_techniques = ['æ¨æµ‹é‡‡æ ·', 'FlashAttentionä¼˜åŒ–', 'æ··åˆç²¾åº¦']
        medium_complexity_techniques = ['KVç¼“å­˜ä¼˜åŒ–', 'SageAttentionæŠ€æœ¯', 'æ¿€æ´»ç¨€ç–åŒ–']
        
        for technique in techniques:
            if any(hct in technique for hct in high_complexity_techniques):
                complexity_score += 3
            elif any(mct in technique for mct in medium_complexity_techniques):
                complexity_score += 2
            else:
                complexity_score += 1
        
        # å®æ–½è¦æ±‚å¤æ‚åº¦
        complex_requirements = ['CUDAå†…æ ¸å¼€å‘', 'æ–°ç®—æ³•å®ç°', 'æ¶æ„çº§åˆ«ä¿®æ”¹']
        for requirement in requirements:
            if any(cr in requirement for cr in complex_requirements):
                complexity_score += 2
        
        if complexity_score >= 8:
            return "æé«˜å¤æ‚åº¦"
        elif complexity_score >= 6:
            return "é«˜å¤æ‚åº¦"
        elif complexity_score >= 4:
            return "ä¸­ç­‰å¤æ‚åº¦"
        else:
            return "ä½å¤æ‚åº¦"
    
    def calculate_priority_score(self, analysis: PaperAnalysis) -> float:
        """è®¡ç®—ä¼˜å…ˆçº§å¾—åˆ†"""
        # æ€§èƒ½æ”¶ç›Šåˆ†æ•°
        benefit_score = len(analysis.performance_improvements) * 2
        
        # å…¼å®¹æ€§åˆ†æ•°
        compatibility_score = analysis.llama_cpp_compatibility * 5
        
        # å¤æ‚åº¦æƒ©ç½š
        complexity_penalties = {
            "ä½å¤æ‚åº¦": 0,
            "ä¸­ç­‰å¤æ‚åº¦": 2,
            "é«˜å¤æ‚åº¦": 4,
            "æé«˜å¤æ‚åº¦": 6
        }
        complexity_penalty = complexity_penalties.get(analysis.estimated_complexity, 3)
        
        # æŠ€æœ¯é‡è¦æ€§åˆ†æ•°
        important_techniques = ['FlashAttentionä¼˜åŒ–', 'KVç¼“å­˜ä¼˜åŒ–', 'INT8é‡åŒ–']
        tech_score = sum(2 for tech in analysis.key_techniques 
                        if any(imp in tech for imp in important_techniques))
        
        total_score = benefit_score + compatibility_score + tech_score - complexity_penalty
        return max(0.0, total_score)
    
    def analyze_single_paper(self, url: str) -> Optional[PaperAnalysis]:
        """åˆ†æå•ç¯‡è®ºæ–‡"""
        print(f"ğŸ” åˆ†æ: {url}")
        
        content_data = self.fetch_paper_content(url)
        if not content_data or not content_data.get('title'):
            self.failed_papers.append(url)
            return None
        
        title = content_data['title']
        abstract = content_data['abstract']
        content = content_data['content']
        
        # æŠ€æœ¯åˆ†æ
        techniques = self.analyze_techniques(title, abstract, content)
        performance = self.extract_performance_data(abstract, content)
        requirements = self.assess_implementation_requirements(techniques, abstract)
        compatibility = self.calculate_llama_cpp_compatibility(techniques, requirements)
        complexity = self.estimate_complexity(techniques, requirements)
        
        # æå–æ½œåœ¨æ”¶ç›Šå’ŒæŒ‘æˆ˜
        benefits = self.extract_potential_benefits(techniques, performance)
        challenges = self.extract_technical_challenges(requirements, complexity)
        
        analysis = PaperAnalysis(
            title=title,
            url=url,
            abstract=abstract[:300] + "..." if len(abstract) > 300 else abstract,
            key_techniques=techniques,
            performance_improvements=performance,
            implementation_requirements=requirements,
            llama_cpp_compatibility=compatibility,
            estimated_complexity=complexity,
            potential_benefits=benefits,
            technical_challenges=challenges,
            priority_score=0.0  # å¾…è®¡ç®—
        )
        
        # è®¡ç®—ä¼˜å…ˆçº§å¾—åˆ†
        analysis.priority_score = self.calculate_priority_score(analysis)
        
        print(f"âœ… {title[:50]}...")
        print(f"   æŠ€æœ¯: {', '.join(techniques[:2])}")
        print(f"   å¤æ‚åº¦: {complexity}")
        print(f"   å…¼å®¹æ€§: {compatibility:.2f}")
        print(f"   ä¼˜å…ˆçº§: {analysis.priority_score:.2f}")
        
        return analysis
    
    def extract_potential_benefits(self, techniques: List[str], performance: List[str]) -> List[str]:
        """æå–æ½œåœ¨æ”¶ç›Š"""
        benefits = []
        
        if any('é‡åŒ–' in t for t in techniques):
            benefits.append('å†…å­˜ä½¿ç”¨å‡å°‘')
            benefits.append('æ¨ç†é€Ÿåº¦æå‡')
        
        if any('FlashAttention' in t for t in techniques):
            benefits.append('æ˜¾å­˜æ•ˆç‡å¤§å¹…æå‡')
        
        if any('ç¼“å­˜' in t for t in techniques):
            benefits.append('é•¿åºåˆ—å¤„ç†èƒ½åŠ›å¢å¼º')
        
        if any('æ¨æµ‹é‡‡æ ·' in t for t in techniques):
            benefits.append('ç”Ÿæˆååé‡æå‡')
        
        if performance:
            benefits.extend([f"å·²éªŒè¯: {p}" for p in performance[:2]])
        
        return benefits if benefits else ['æ€§èƒ½ä¼˜åŒ–']
    
    def extract_technical_challenges(self, requirements: List[str], complexity: str) -> List[str]:
        """æå–æŠ€æœ¯æŒ‘æˆ˜"""
        challenges = []
        
        if 'CUDAå†…æ ¸å¼€å‘' in requirements:
            challenges.append('éœ€è¦GPUç¼–ç¨‹ä¸“ä¸šçŸ¥è¯†')
        
        if 'æ¶æ„çº§åˆ«ä¿®æ”¹' in requirements:
            challenges.append('å¯èƒ½å½±å“ç³»ç»Ÿç¨³å®šæ€§')
        
        if 'æ–°ç®—æ³•å®ç°' in requirements:
            challenges.append('ç®—æ³•è°ƒä¼˜å’ŒéªŒè¯å¤æ‚')
        
        if complexity in ['é«˜å¤æ‚åº¦', 'æé«˜å¤æ‚åº¦']:
            challenges.append('å®æ–½å‘¨æœŸé•¿')
            challenges.append('éœ€è¦ä¸“ä¸šå›¢é˜Ÿ')
        
        return challenges if challenges else ['å¸¸è§„å®æ–½æŒ‘æˆ˜']
    
    def run_comprehensive_analysis(self) -> Dict:
        """è¿è¡Œå…¨é¢åˆ†æ"""
        print("ğŸš€ å¼€å§‹å…¨é¢è®ºæ–‡åˆ†æ")
        print("="*60)
        
        # ä»paper.mdè¯»å–æ‰€æœ‰arxivé“¾æ¥
        try:
            with open('/root/llama.cpp-clip/PRPs/paper_analysis/paper.md', 'r', encoding='utf-8') as f:
                content = f.read()
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ°paper.mdæ–‡ä»¶")
            return {}
        
        # æå–æ‰€æœ‰arxiv URL
        arxiv_urls = re.findall(r'https://arxiv\.org/[^\s]+', content)
        print(f"ğŸ“š å‘ç° {len(arxiv_urls)} ç¯‡arxivè®ºæ–‡")
        print()
        
        # åˆ†ææ¯ç¯‡è®ºæ–‡
        for i, url in enumerate(arxiv_urls, 1):
            print(f"[{i}/{len(arxiv_urls)}]", end=" ")
            analysis = self.analyze_single_paper(url)
            if analysis:
                self.analyses.append(analysis)
            print()
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.analyses.sort(key=lambda x: x.priority_score, reverse=True)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        report = self.generate_comprehensive_report()
        
        return report
    
    def generate_comprehensive_report(self) -> Dict:
        """ç”Ÿæˆå…¨é¢æŠ¥å‘Š"""
        return {
            "analysis_metadata": {
                "timestamp": datetime.now().isoformat(),
                "method": "åŸºäºçœŸå®è®ºæ–‡å†…å®¹çš„å…¨é¢åˆ†æ",
                "papers_found": len(self.analyses) + len(self.failed_papers),
                "papers_analyzed": len(self.analyses),
                "failed_papers": len(self.failed_papers)
            },
            "top_priority_papers": [
                {
                    "rank": i+1,
                    "title": analysis.title,
                    "priority_score": analysis.priority_score,
                    "key_techniques": analysis.key_techniques,
                    "complexity": analysis.estimated_complexity,
                    "compatibility": analysis.llama_cpp_compatibility,
                    "benefits": analysis.potential_benefits[:3],
                    "url": analysis.url
                }
                for i, analysis in enumerate(self.analyses[:10])
            ],
            "technical_categories": self.categorize_by_technique(),
            "complexity_distribution": self.analyze_complexity_distribution(),
            "implementation_roadmap": self.generate_implementation_roadmap(),
            "detailed_analyses": [asdict(analysis) for analysis in self.analyses],
            "failed_urls": self.failed_papers
        }
    
    def categorize_by_technique(self) -> Dict[str, List[str]]:
        """æŒ‰æŠ€æœ¯ç±»å‹åˆ†ç±»"""
        categories = {}
        for analysis in self.analyses:
            for technique in analysis.key_techniques:
                if technique not in categories:
                    categories[technique] = []
                categories[technique].append(analysis.title)
        return categories
    
    def analyze_complexity_distribution(self) -> Dict[str, int]:
        """åˆ†æå¤æ‚åº¦åˆ†å¸ƒ"""
        distribution = {}
        for analysis in self.analyses:
            complexity = analysis.estimated_complexity
            distribution[complexity] = distribution.get(complexity, 0) + 1
        return distribution
    
    def generate_implementation_roadmap(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆå®æ–½è·¯çº¿å›¾"""
        roadmap = {
            "ç¬¬ä¸€é˜¶æ®µ (ç«‹å³å®æ–½)": [],
            "ç¬¬äºŒé˜¶æ®µ (ä¸­æœŸè§„åˆ’)": [],
            "ç¬¬ä¸‰é˜¶æ®µ (é•¿æœŸç ”ç©¶)": []
        }
        
        for analysis in self.analyses:
            if (analysis.priority_score >= 8 and 
                analysis.estimated_complexity in ["ä½å¤æ‚åº¦", "ä¸­ç­‰å¤æ‚åº¦"]):
                roadmap["ç¬¬ä¸€é˜¶æ®µ (ç«‹å³å®æ–½)"].append(analysis.title)
            elif analysis.priority_score >= 6:
                roadmap["ç¬¬äºŒé˜¶æ®µ (ä¸­æœŸè§„åˆ’)"].append(analysis.title)
            else:
                roadmap["ç¬¬ä¸‰é˜¶æ®µ (é•¿æœŸç ”ç©¶)"].append(analysis.title)
        
        return roadmap

def main():
    analyzer = ComprehensivePaperAnalyzer()
    report = analyzer.run_comprehensive_analysis()
    
    # ä¿å­˜å®Œæ•´æŠ¥å‘Š
    output_path = '/root/llama.cpp-clip/PRPs/paper_analysis/results/comprehensive_analysis.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*60)
    print("ğŸ¯ åˆ†æå®Œæˆæ€»ç»“:")
    print(f"âœ… æˆåŠŸåˆ†æ: {report['analysis_metadata']['papers_analyzed']} ç¯‡")
    print(f"âŒ åˆ†æå¤±è´¥: {report['analysis_metadata']['failed_papers']} ç¯‡")
    print(f"\nğŸ“Š TOP 5 ä¼˜å…ˆçº§è®ºæ–‡:")
    
    for paper in report['top_priority_papers'][:5]:
        print(f"{paper['rank']}. {paper['title'][:60]}...")
        print(f"   å¾—åˆ†: {paper['priority_score']:.1f} | å¤æ‚åº¦: {paper['complexity']} | å…¼å®¹æ€§: {paper['compatibility']:.2f}")
    
    print(f"\nğŸ’¾ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

if __name__ == "__main__":
    main()