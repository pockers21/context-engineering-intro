#!/usr/bin/env python3
"""
åŸºäºçœŸå®è®ºæ–‡å†…å®¹çš„æ¨ç†ä¼˜åŒ–åˆ†æå·¥å…·
é€šè¿‡ç½‘ç»œè®¿é—®è·å–è®ºæ–‡å®é™…å†…å®¹ï¼Œæä¾›å‡†ç¡®çš„æŠ€æœ¯è¯„ä¼°
"""

import subprocess
import re
import json
from datetime import datetime
from dataclasses import dataclass
from typing import List, Dict, Optional

@dataclass 
class RealPaperAnalysis:
    title: str
    url: str
    abstract: str
    key_contributions: List[str]
    technical_details: List[str]
    performance_claims: List[str]
    implementation_complexity: str
    llama_cpp_relevance: float

class RealPaperAnalyzer:
    def __init__(self):
        self.papers = []
        self.analysis_results = {}
    
    def fetch_arxiv_content(self, url: str) -> Dict[str, str]:
        """è·å–arxivè®ºæ–‡çš„å®é™…å†…å®¹"""
        try:
            cmd = f'curl -s "{url}"'
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            html_content = result.stdout
            
            # æå–æ ‡é¢˜
            title_match = re.search(r'<title>(.*?)</title>', html_content)
            title = title_match.group(1).strip() if title_match else "æœªæ‰¾åˆ°æ ‡é¢˜"
            
            # æå–æ‘˜è¦
            abstract_match = re.search(r'<div class="ltx_abstract">.*?<p class="ltx_p"[^>]*>(.*?)</p>', html_content, re.DOTALL)
            abstract = ""
            if abstract_match:
                abstract = re.sub(r'<[^>]+>', '', abstract_match.group(1))
                abstract = re.sub(r'\s+', ' ', abstract).strip()
            
            return {
                "title": title,
                "abstract": abstract,
                "html_content": html_content
            }
        except Exception as e:
            print(f"è·å–è®ºæ–‡å†…å®¹å¤±è´¥ {url}: {e}")
            return {"title": "", "abstract": "", "html_content": ""}
    
    def analyze_paper_from_list(self, paper_line: str) -> Optional[RealPaperAnalysis]:
        """ä»è®ºæ–‡åˆ—è¡¨ä¸­çš„ä¸€è¡Œåˆ†æçœŸå®è®ºæ–‡å†…å®¹"""
        
        # æå–URL
        url_match = re.search(r'https://arxiv\.org/[^\s]+', paper_line)
        if not url_match:
            return None
            
        url = url_match.group(0)
        print(f"æ­£åœ¨åˆ†æè®ºæ–‡: {url}")
        
        # è·å–çœŸå®å†…å®¹
        content = self.fetch_arxiv_content(url)
        if not content["title"]:
            return None
        
        # åŸºäºçœŸå®å†…å®¹åˆ†æ
        title = content["title"]
        abstract = content["abstract"]
        
        # åˆ†ææŠ€æœ¯è´¡çŒ®
        key_contributions = self.extract_contributions(abstract, content["html_content"])
        
        # åˆ†ææ€§èƒ½å£°æ˜
        performance_claims = self.extract_performance_claims(abstract)
        
        # è¯„ä¼°å®æ–½å¤æ‚åº¦
        complexity = self.assess_implementation_complexity(title, abstract, key_contributions)
        
        # è¯„ä¼°ä¸llama.cppçš„ç›¸å…³æ€§
        relevance = self.assess_llama_cpp_relevance(title, abstract, key_contributions)
        
        return RealPaperAnalysis(
            title=title,
            url=url,
            abstract=abstract,
            key_contributions=key_contributions,
            technical_details=[],  # éœ€è¦è¿›ä¸€æ­¥åˆ†æHTMLå†…å®¹
            performance_claims=performance_claims,
            implementation_complexity=complexity,
            llama_cpp_relevance=relevance
        )
    
    def extract_contributions(self, abstract: str, html_content: str) -> List[str]:
        """ä»æ‘˜è¦å’Œå†…å®¹ä¸­æå–æŠ€æœ¯è´¡çŒ®"""
        contributions = []
        
        # ä»æ‘˜è¦ä¸­æå–å…³é”®æŠ€æœ¯ç‚¹
        if "introduce" in abstract.lower():
            intro_part = abstract.lower().split("introduce")[1].split(".")[0]
            contributions.append(f"æ ¸å¿ƒæŠ€æœ¯: {intro_part.strip()}")
        
        # æŸ¥æ‰¾æ–¹æ³•å…³é”®è¯
        methods = []
        if "speculative" in abstract.lower():
            methods.append("æ¨æµ‹é‡‡æ ·æŠ€æœ¯")
        if "quantization" in abstract.lower():
            methods.append("é‡åŒ–æŠ€æœ¯")  
        if "attention" in abstract.lower():
            methods.append("æ³¨æ„åŠ›ä¼˜åŒ–")
        if "cache" in abstract.lower():
            methods.append("ç¼“å­˜ä¼˜åŒ–")
        
        contributions.extend(methods)
        return contributions
    
    def extract_performance_claims(self, abstract: str) -> List[str]:
        """æå–æ€§èƒ½æå‡å£°æ˜"""
        claims = []
        
        # æŸ¥æ‰¾åŠ é€Ÿæ¯”
        speedup_matches = re.findall(r'(\d+\.?\d*)\s*x\s*(speedup|improvement|faster)', abstract.lower())
        for match in speedup_matches:
            claims.append(f"{match[0]}x {match[1]}")
        
        # æŸ¥æ‰¾ç™¾åˆ†æ¯”æå‡
        percent_matches = re.findall(r'(\d+)%\s*(improvement|better|reduction)', abstract.lower())
        for match in percent_matches:
            claims.append(f"{match[0]}% {match[1]}")
            
        return claims
    
    def assess_implementation_complexity(self, title: str, abstract: str, contributions: List[str]) -> str:
        """è¯„ä¼°å®æ–½å¤æ‚åº¦"""
        complexity_indicators = 0
        
        # æŠ€æœ¯å¤æ‚åº¦æŒ‡æ ‡
        if any(word in title.lower() + abstract.lower() for word in ["novel", "new", "training", "fine-tuning"]):
            complexity_indicators += 2
        
        if any(word in title.lower() + abstract.lower() for word in ["kernel", "cuda", "optimization"]):
            complexity_indicators += 1
            
        if any(word in title.lower() + abstract.lower() for word in ["framework", "system", "architecture"]):
            complexity_indicators += 2
        
        if complexity_indicators >= 4:
            return "é«˜å¤æ‚åº¦"
        elif complexity_indicators >= 2:
            return "ä¸­ç­‰å¤æ‚åº¦"
        else:
            return "ä½å¤æ‚åº¦"
    
    def assess_llama_cpp_relevance(self, title: str, abstract: str, contributions: List[str]) -> float:
        """è¯„ä¼°ä¸llama.cppçš„ç›¸å…³æ€§ (0-1)"""
        relevance_score = 0.0
        
        # ç›´æ¥ç›¸å…³çš„æŠ€æœ¯
        if any(word in title.lower() + abstract.lower() for word in 
               ["inference", "generation", "sampling", "decoding"]):
            relevance_score += 0.3
        
        if any(word in title.lower() + abstract.lower() for word in
               ["quantization", "compression", "acceleration"]):
            relevance_score += 0.3
            
        if any(word in title.lower() + abstract.lower() for word in
               ["attention", "transformer", "llm"]):
            relevance_score += 0.2
        
        if any(word in title.lower() + abstract.lower() for word in
               ["gpu", "cuda", "kernel", "memory"]):
            relevance_score += 0.2
        
        return min(relevance_score, 1.0)

def main():
    """åˆ†æè®ºæ–‡åˆ—è¡¨ä¸­çš„çœŸå®å†…å®¹"""
    print("ğŸ” å¼€å§‹åŸºäºçœŸå®è®ºæ–‡å†…å®¹çš„åˆ†æ")
    print("="*50)
    
    analyzer = RealPaperAnalyzer()
    
    # ä»paper.mdè¯»å–è®ºæ–‡åˆ—è¡¨
    try:
        with open('/root/llama.cpp-clip/PRPs/paper_analysis/paper.md', 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        print("æœªæ‰¾åˆ°paper.mdæ–‡ä»¶")
        return
    
    # æå–arxivé“¾æ¥å¹¶åˆ†æ
    arxiv_urls = re.findall(r'https://arxiv\.org/[^\s]+', content)
    
    print(f"å‘ç° {len(arxiv_urls)} ä¸ªarxivè®ºæ–‡é“¾æ¥")
    print()
    
    real_analyses = []
    for i, url in enumerate(arxiv_urls[:5], 1):  # å…ˆåˆ†æå‰5ç¯‡
        print(f"[{i}/5] åˆ†æ: {url}")
        analysis = analyzer.analyze_paper_from_list(url)
        if analysis:
            real_analyses.append(analysis)
            print(f"âœ… æ ‡é¢˜: {analysis.title[:60]}...")
            print(f"   å¤æ‚åº¦: {analysis.implementation_complexity}")
            print(f"   ç›¸å…³æ€§: {analysis.llama_cpp_relevance:.2f}")
            print(f"   æ€§èƒ½å£°æ˜: {', '.join(analysis.performance_claims[:2])}")
        else:
            print(f"âŒ åˆ†æå¤±è´¥")
        print()
    
    # ä¿å­˜çœŸå®åˆ†æç»“æœ
    results = {
        "analysis_timestamp": datetime.now().isoformat(),
        "method": "åŸºäºçœŸå®è®ºæ–‡å†…å®¹åˆ†æ",
        "papers_analyzed": len(real_analyses),
        "real_analyses": [
            {
                "title": p.title,
                "url": p.url,
                "abstract": p.abstract[:200] + "..." if len(p.abstract) > 200 else p.abstract,
                "key_contributions": p.key_contributions,
                "performance_claims": p.performance_claims,
                "implementation_complexity": p.implementation_complexity,
                "llama_cpp_relevance": p.llama_cpp_relevance
            } for p in real_analyses
        ]
    }
    
    with open('/root/llama.cpp-clip/PRPs/paper_analysis/results/real_paper_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("ğŸ“Š åŸºäºçœŸå®å†…å®¹çš„åˆ†æå®Œæˆ!")
    print("ç»“æœå·²ä¿å­˜åˆ° real_paper_analysis.json")

if __name__ == "__main__":
    main()